{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISV_j8bo9if8"
   },
   "source": [
    "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
    "<h3 style=\"text-align: center;\"><b>Phystech School of Applied Mathematics and Informatics (PSAMI) MIPT</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnuMlpJp9if9"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkoop-MT9if-"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Neuron with sigmoid</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qGKppuWS9if-"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gfbXqXQ9if_"
   },
   "source": [
    "### You should solve notebook `[seminar]perceptron.ipynb` first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUjxg_IS31tn"
   },
   "source": [
    "In this notebook you will learn how to: \n",
    "- implement class **`Neuron()`** with sigmoid activation function\n",
    "- train and validate this class on generated and real data (files with real data are in /data folder)\n",
    "- compare quality of your model with models from module `scikit-learn` (`sklearn.linear_model.Perceptron()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHap62ES9igB"
   },
   "source": [
    "In this notebook you will implement neuron with various activation functions: Sigmoid, ReLU, LeakyReLU и ELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-OYlV519igB"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OU1KkHre9igF"
   },
   "source": [
    "As a reminder, **sigmoid curve (sigmoid)** looks like this:  \n",
    "    \n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sG06ukL831t-"
   },
   "source": [
    "Its plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "I8-Pa5CL31uD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-5, 5, 100), 1 / (1 + np.exp(-np.linspace(-5, 5, 100))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3B2wpFb31uH"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygSraLzH31uJ"
   },
   "source": [
    "**Task 1**\n",
    "\n",
    "Derive a derivative of this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmEexUY631uM"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cgpFOaPm9igG"
   },
   "source": [
    "In this case we are facing a binary classification problem again. Let's use the same loss function **mean square error**, but instread of threshold activation we'll use sigmoid:\n",
    "\n",
    "$$MSE\\_Loss(\\hat{y}, y) = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n} (\\sigma(w \\cdot X_i) - y_i)^2$$ \n",
    " \n",
    "\n",
    "Here $w \\cdot X_i$ - dot product, and $\\sigma(w \\cdot X_i) =\\frac{1}{1+e^{-w \\cdot X_i}} $ - sigmoid ($i$ -- object's number in dataset).  \n",
    "\n",
    "**Note:** It is supposed, that $b$ - free term - is a part of weights vector: $w_0$. So, if we add column of ones to the left side of $X$, we will get $b$ as a free term in dot product (figure out why it works on a piece of paper -- you will easily get it). But in our implementation of `Perceptron()` let's calculate $b$ separately (to make it clearer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUp-NwTw9igG"
   },
   "source": [
    "We will take a derivative of loss by weights and descend in weights space in the direction of the fastest loss decrease. Formula of updating weights in gradient descenе:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^{j+1} = w^{j} - \\alpha \\frac{\\partial Loss}{\\partial w} (w^{j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^j$ -- weights vector in $j$-th iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unwrap it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For weight $w_j$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial Loss}{\\partial w_j} = \n",
    "\\frac{2}{n} \\sum_{i=1}^n \\left(\\sigma(w \\cdot x_i) - y_i\\right)(\\sigma(w \\cdot x_i))_{w_j}' = \\frac{2}{n} \\sum_{i=1}^n \\left(\\sigma(w \\cdot x_i) - y_i\\right)\\sigma(w \\cdot x_i)(1 - \\sigma(w \\cdot x_i))x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient of $Loss$ by weights vector is a vector, $j$-th component of which equals $\\frac{\\partial Loss}{\\partial w_j}$ (again, weights number is $m$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "    \\frac{\\partial Loss}{\\partial w} &= \\begin{bmatrix}\n",
    "           \\frac{2}{n} \\sum_{i=1}^n \\left(\\sigma(w \\cdot x_i) - y_i\\right)\\sigma(w \\cdot x_i)(1 - \\sigma(w \\cdot x_i))x_{i1} \\\\\n",
    "           \\frac{2}{n} \\sum_{i=1}^n \\left(\\sigma(w \\cdot x_i) - y_i\\right)\\sigma(w \\cdot x_i)(1 - \\sigma(w \\cdot x_i))x_{i2} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\frac{2}{n} \\sum_{i=1}^n \\left(\\sigma(w \\cdot x_i) - y_i\\right)\\sigma(w \\cdot x_i)(1 - \\sigma(w \\cdot x_i))x_{im}\n",
    "         \\end{bmatrix}\n",
    "\\end{align}=\\frac{1}{n} X^T (\\sigma(w \\cdot X) - y)\\sigma(w \\cdot X)(1 - \\sigma(w \\cdot X))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1kKhH1v9igI"
   },
   "source": [
    "Implement sigmoid and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCgAeho19igI"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nXwsy-7J9igL"
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid function\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKurn-7F9igN"
   },
   "source": [
    "Now it's time to write a neuron with sigmoid activation. Code here will be almost the same as in perceptron, but weights will update differently and different activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 610,
     "status": "error",
     "timestamp": 1539422231000,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "AM9vn3OX9igO",
    "outputId": "0391ccb7-3655-4637-e1eb-314d67b164e4"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, w=None, b=0):\n",
    "        \"\"\"\n",
    "        :param: w -- weights vector\n",
    "        :param: b -- bias scalar\n",
    "        \"\"\"\n",
    "        # Let's leave an opportunity for a user to set weights and biases directly\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def activate(self, x):\n",
    "        # You code here\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        This function computes an answer of the perceptron given a set of objects\n",
    "        :param: X -- matrix of objects sized (n, m), every row - separate object\n",
    "        :return: vector sized (n, 1) of zeros and ones containing model answers \n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        y_pred = np.zeros((n, 1))  # y_pred(icted) - predicted classes\n",
    "        \n",
    "        # You code here\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n",
    "        \"\"\"\n",
    "        Updates weights values given objects\n",
    "        :param: X -- matrix of objects sized (n, m)\n",
    "                y -- right answers vector sized (n, 1)\n",
    "                learning_rate - \"speed of learning\" (symbol alpha in formulas above)\n",
    "        This method doesn't return anything, it only corrects weights using gradient\n",
    "        descend.\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "    \n",
    "    def fit(self, X, y, num_epochs=300):\n",
    "        \"\"\"\n",
    "        Descend in a minimum\n",
    "        :param: X -- matrix of objects sized (n, m)\n",
    "                y -- right answers vector sized (n, 1)\n",
    "                num_epochs -- number of training steps\n",
    "        :return: Loss_values -- vector of loss values\n",
    "        \"\"\"\n",
    "        self.w = np.zeros((X.shape[1], 1))  # column (m, 1)\n",
    "        self.b = 0  # bias (free term)\n",
    "        losses = []  # loss values on every step of fitting\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            # Your code here\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thtFp-at9igS"
   },
   "source": [
    "### Neuron testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOuYzf_u9igS"
   },
   "source": [
    "Here your goal is to test our new neuron **on the same data (\"Apples and pears\")** the same way as you did with perceptron (you can freely copy code, but be aware, some of code may be different).\n",
    "As the result your goal is to get: \n",
    "* plot, showing changes of $Loss$ function depending on iteratins number\n",
    "* plot with coloring of dataset by neuron with sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JGs_F5N331u9"
   },
   "source": [
    "**Check forward_pass()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 660,
     "status": "error",
     "timestamp": 1539421983029,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "aKmrhe_831vG",
    "outputId": "0bc0b4ae-084b-463e-9602-76f2cd17a260"
   },
   "outputs": [],
   "source": [
    "w = np.array([1., 2.]).reshape(2, 1)\n",
    "b = 2.\n",
    "X = np.array([[1., 3.],\n",
    "              [2., 4.],\n",
    "              [-1., -3.2]])\n",
    "\n",
    "neuron = Neuron(w, b)\n",
    "y_pred = neuron.forward_pass(X)\n",
    "print (\"y_pred = \" + str(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWjbXaT431vN"
   },
   "source": [
    "|Should be||\n",
    "|------|-------|\n",
    "|**y_pred**|[0.99987661, 0.99999386,0.00449627]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYG2uFcy31vP"
   },
   "source": [
    "**Check backward_pass()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvUkrgQy31vQ"
   },
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 1]).reshape(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 577,
     "status": "error",
     "timestamp": 1539421988792,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -300
    },
    "id": "pWooOLya31vX",
    "outputId": "cd2b2358-b615-4413-e984-0966ede7f355"
   },
   "outputs": [],
   "source": [
    "neuron.backward_pass(X, y, y_pred)\n",
    "\n",
    "print (\"w = \" + str(neuron.w))\n",
    "print (\"b = \" + str(neuron.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcVff1Is31vc"
   },
   "source": [
    "|Shold be||\n",
    "|------|-------|\n",
    "|**w**|[0.99985106, 1.99952388]|\n",
    "|**b**|2.000148326741343|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how it performs on the data (use cells from Perceptron notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWgwECpD9igT"
   },
   "outputs": [],
   "source": [
    "# You code here (use multiple cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PfNPtrZ31vq"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iT8so51w31wD"
   },
   "source": [
    "In **homework** you will need to implement `Neuron()` with other activation functions: ReLu, LeakyReLU, ELU, SeLU, Swish (some of them). Weights and values of loss functions will be checked in the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhGNsWPQ9igf"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Useful links</b></h3>\n",
    "\n",
    "0). You must check this artcile by Stanford: http://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "1). Great article on activation functions: https://www.jeremyjordan.me/neural-networks-activation-functions/\n",
    "\n",
    "2). [Video by Siraj Raval](https://www.youtube.com/watch?v=-7scQpJT7uo)\n",
    "\n",
    "3). Modern paper on activation functions. One of the hype functions is $swish(x) = x\\sigma (\\beta x)$: https://arxiv.org/pdf/1710.05941.pdf (by the way, *neural acrhitecture search* was used in search of this function)\n",
    "\n",
    "4). **SeLU** has some interesting properties, proven with probability theory: https://arxiv.org/pdf/1706.02515.pdf (yes, this paper consists of 102 pages)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]neuron.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
