{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISV_j8bo9if8"
   },
   "source": [
    "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
    "<h3 style=\"text-align: center;\"><b>Phystech School of Applied Mathematics and Informatics (PSAMI) MIPT</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnuMlpJp9if9"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkoop-MT9if-"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Homework: neuron with various activation functions</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qGKppuWS9if-"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gfbXqXQ9if_"
   },
   "source": [
    "### You need to solve first `[seminar]perceptron.ipynb` and `[seminar]neuron.ipynb`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUjxg_IS31tn"
   },
   "source": [
    "**It is a frequently asked question: which activation function should I choose?** In this notebook we suggest finding out the truth and compare neurons with various activation functions (their quality on two datasets). Make sure all of the experiments are conducted in the same conditions (otherwise an experement will not be fair).\n",
    "\n",
    "In this task you will: \n",
    "- implement class **`Neuron()`** with various activation functions\n",
    "- train and validate your class on generated and real data (files with real data are in '/data' folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHap62ES9igB"
   },
   "source": [
    "In this notebook you will implement neuron with various activation functions: Sigmoid, ReLU, LeakyReLU and ELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-OYlV519igB"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap  # here some magic things for colorization are lying\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1VCt_rDHum3"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42  # do not change, results of the test depend on this!\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmEexUY631uM"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cgpFOaPm9igG"
   },
   "source": [
    "In this case we are facing a binary classification problem again. Let's use the same loss function **mean square error**, but instread of threshold activation we'll use sigmoid:\n",
    "\n",
    "$$MSE\\_Loss(\\hat{y}, y) = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n} (\\sigma(w \\cdot X_i) - y_i)^2$$ \n",
    " \n",
    "\n",
    "Here $w \\cdot X_i$ - dot product, and $\\sigma(w \\cdot X_i) =\\frac{1}{1+e^{-w \\cdot X_i}} $ - sigmoid ($i$ -- object's number in dataset).  \n",
    "\n",
    "**Note:** It is supposed, that $b$ - free term - is a part of weights vector: $w_0$. So, if we add column of ones to the left side of $X$, we will get $b$ as a free term in dot product (figure out why it works on a piece of paper -- you will easily get it). But in our implementation of `Perceptron()` let's calculate $b$ separately (to make it clearer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yn3F0layHunD"
   },
   "outputs": [],
   "source": [
    "def Loss(y_pred, y):\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    return 0.5 * np.mean((y_pred - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYoQAdXoHunK"
   },
   "source": [
    "Futher there are several activation functions, and you need to implement a class `Neuron` similarly with how it was in seminars. The principle is the same, but the formula for updating the weights and the for the predicting function.\n",
    "\n",
    "**The rules are simple**: There are three activation functions, the first have all the formuals, you only need to code them. In the second will be written derivative, but it will not be substituted in $Loss$, this is task for you. The third will have only function formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHvdwJW9HunN"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Neuron with ReLU (Recitified Linear Unit)</b></h2>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMA7Gi6KHunQ"
   },
   "source": [
    "ReLU is the most frequently used (at least couple years ago) activation function in neural networks. It looks very simple:\n",
    "\n",
    "\\begin{equation*}\n",
    "ReLU(x) =\n",
    " \\begin{cases}\n",
    "   0, &\\text{$x \\le 0$}\\\\\n",
    "   x, &\\text{$x \\gt 0$}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Or:\n",
    "\n",
    "$$\n",
    "ReLU(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "We just restrict negative numbers to go.\n",
    "\n",
    "The derivative here is taken as a derivative of a piecewise-given function (at zero, it is defined by zero):\n",
    "\n",
    "\\begin{equation*}\n",
    "ReLU'(x) = \n",
    " \\begin{cases}\n",
    "   0, &\\text{$x \\le 0$}\\\\\n",
    "   1, &\\text{$x \\gt 0$}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Graph of this function and graph of its derivative:\n",
    "\n",
    "<img src=\"https://upload-images.jianshu.io/upload_images/1828517-0828da0d1164c024.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" width=800 height=400>\n",
    "\n",
    "Substitute ReLU in Loss:\n",
    "\n",
    "$$Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (ReLU(w \\cdot X_i) - y_i)^2 = \\begin{equation*}\n",
    "\\frac{1}{2n}\\sum_{i=1}^{n}\n",
    " \\begin{cases}\n",
    "    y_i^2, &{w \\cdot X_i \\le 0}\\\\\n",
    "   (w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\gt 0}\n",
    " \\end{cases}\n",
    "\\end{equation*}$$  \n",
    "\n",
    "(remember that $w \\cdot X_i$ -- is a number in this case (result of dot products of two vectors).\n",
    "\n",
    "Then the formula for updating weight in gradient descend will be as follows (in a matrix for, we suggest you do it yourself. It derives from a fomula for one object.)\n",
    "\n",
    "$$ \\frac{\\partial Loss}{\\partial w} = \\begin{equation*}\n",
    "\\sum_{i=1}^{n}\n",
    " \\begin{cases}\n",
    "   0, &{w \\cdot X_i \\le 0}\\\\\n",
    "   \\frac{1}{n} X_i^T (w \\cdot X_i - y), &{w \\cdot X_i \\gt 0}\n",
    " \\end{cases}\n",
    "\\end{equation*}$$\n",
    "\n",
    "(remember that $w \\cdot X$ here is a product of matrix and vector $w$ (vector -- is a matrix too, isn't it?) and matrix $X$ )\n",
    "\n",
    "Why in the first case it is 0? Because weights are not included in $y_i^2$, and we take derivative exactly by weights $w$.\n",
    "\n",
    "* Implement ReLU and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCgAeho19igI"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"ReLU\"\"\"\n",
    "    return <Your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nXwsy-7J9igL"
   },
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU\"\"\"\n",
    "    return <Your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKurn-7F9igN"
   },
   "source": [
    "Now you need to code neuron with ReLU. Here it's all similar to Perceptron, but the weights are updated differently and the activation function is different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AM9vn3OX9igO"
   },
   "outputs": [],
   "source": [
    "class NeuronReLU:\n",
    "    def __init__(self, w=None, b=0):\n",
    "        \"\"\"\n",
    "        :param: w -- weights vector\n",
    "        :param: b -- bias scalar\n",
    "        \"\"\"\n",
    "        # Let's leave an opportunity for a user to set weights and biases directly\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def activate(self, x):\n",
    "        # You code here\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        This function computes an answer of the perceptron given a set of objects\n",
    "        :param: X -- matrix of objects sized (n, m), every row - separate object\n",
    "        :return: vector sized (n, 1) of zeros and ones containing model answers \n",
    "        \"\"\"\n",
    "        # You code here\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        y_pred = np.zeros((n, 1))  # y_pred == y_predicted - predicted classes\n",
    "        # You code here\n",
    "    \n",
    "    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n",
    "        \"\"\"\n",
    "        Updates weights values given objects\n",
    "        :param: X -- matrix of objects sized (n, m)\n",
    "                y -- right answers vector sized (n, 1)\n",
    "                learning_rate - \"speed of learning\" (symbol alpha in formulas above)\n",
    "        This method doesn't return anything, it only corrects weights using gradient\n",
    "        descend.\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        # You code here\n",
    "    \n",
    "    def fit(self, X, y, num_epochs=300):\n",
    "        \"\"\"\n",
    "        Descend in a minimum\n",
    "        :param: X -- matrix of objects sized (n, m)\n",
    "                y -- right answers vector sized (n, 1)\n",
    "                num_epochs -- number of training steps\n",
    "        :return: Loss_values -- vector of loss values\n",
    "        \"\"\"\n",
    "        self.w = np.zeros((X.shape[1], 1))  # column (m, 1)\n",
    "        self.b = 0  # bias (number)\n",
    "        Loss_values = []  # loss values on every step of fitting\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            # You code here\n",
    "        \n",
    "        return Loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thtFp-at9igS"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Testing neuron with ReLU</b></h3>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOuYzf_u9igS"
   },
   "source": [
    "Here your task is to test your neuron **on the same dataset (\"Apples and pears\")** similarly with the way, how this was made with perceptron (you can freely copy your code, but be careful - something yet need to be corrected).\n",
    "As the result you need to display: \n",
    "* graph showing how loss function $Loss$ changes depending on iterations number\n",
    "* graph with coloring of the dataset by sigmoidal neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chEeb88gHuny"
   },
   "source": [
    "***Note***: please, check `.shape` of matricies and vectors more often: `self.w`, `X` and `y` inside the class. Often mistake is solved with transposition or with method `.reshape()`. Don't forget to check what vector (what size) you want to get as an output -- this quite helps not to get confused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JGs_F5N331u9"
   },
   "source": [
    "**(for the test) Check forward_pass()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKmrhe_831vG"
   },
   "outputs": [],
   "source": [
    "w = np.array([1., 2.]).reshape(2, 1)\n",
    "b = 2.\n",
    "X = np.array([[1., 3.],\n",
    "              [2., 4.],\n",
    "              [-1., -3.2]])\n",
    "\n",
    "neuron = NeuronReLU(w, b)\n",
    "y_pred = neuron.forward_pass(X)\n",
    "print (\"y_pred = \" + str(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYG2uFcy31vP"
   },
   "source": [
    "**(for the test) Check backward_pass()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvUkrgQy31vQ"
   },
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 1]).reshape(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWooOLya31vX"
   },
   "outputs": [],
   "source": [
    "neuron.backward_pass(X, y, y_pred)\n",
    "\n",
    "print (\"w = \" + str(neuron.w))\n",
    "print (\"b = \" + str(neuron.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X085TCvBHuoQ"
   },
   "source": [
    "\"Apples and pears\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEnSapY_HuoR"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/apples_pears.csv\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data['target'], cmap='rainbow')\n",
    "plt.title('Apples and pears', fontsize=15)\n",
    "plt.xlabel('symmetry', fontsize=14)\n",
    "plt.ylabel('yellowness', fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03cIoaEsHuod"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,:2].values  # matrix objects-features\n",
    "y = data['target'].values.reshape((-1, 1))  # classes (column of zeros and ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dR6xZg6kHuol"
   },
   "source": [
    "Display loss during the training of a nueron with ReLU on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZDtNG6CHuop"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "neuron = <You code here>\n",
    "Loss_values = <Your code here>\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(Loss_values)\n",
    "plt.title('Loss function', fontsize=15)\n",
    "plt.xlabel('iteration number', fontsize=14)\n",
    "plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPxo2YVeHuou"
   },
   "source": [
    "Probably your loss is a straight line now, and you can see that weights are not updated. But why?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXUmEeUBHuov"
   },
   "source": [
    "Everything is simple -- possibly we have not yet told you, but if you look closely, you can see that self.w and self.b are initialized with zeros in the beginning of the method `.fit()`. If you write it down on the paper how the update is procceed, you will see that because of ReLU weights are simply will not change when you initialize them with zeros.\n",
    "\n",
    "This is one of the reasons why they initialize weights with random numbers in neural networks (usually from [0, 1)).\n",
    "\n",
    "Let's train a neuron but initialize weights at the beginning (do 10000 iterations). \n",
    "\n",
    "**!!! Comment out the initialization with zeros in the function `.fit()` of the class `NeuronReLU` !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E821cUM0Huo8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "neuron = NeuronReLU(w=np.random.rand(X.shape[1], 1), b=np.random.rand(1))\n",
    "Loss_values = neuron.fit(X, y, num_epochs=10000)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(Loss_values)\n",
    "plt.title('Loss function', fontsize=15)\n",
    "plt.xlabel('iteration number', fontsize=14)\n",
    "plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3XeufnfHupE"
   },
   "source": [
    "**(for the test) Check loss:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCn5DSP5HupK"
   },
   "source": [
    "Display a summ of the first five and the last five values of loss during training for num_epochs=10000, round to the 4-th decimal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gThPeHNm9Am6"
   },
   "source": [
    "IMPORTANT! If you have run the previous cell with code several times, then to carry the results in the test in Canvas, please, run this cell from \"zero\", restart Runtime, since `random` brings in randomness. There are several possible answers, in case, you have run it several times, but it's better not to carry the result after 10 runs of these and following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELZVl_9rHupL"
   },
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkWMrc9uHupS"
   },
   "source": [
    "Let's see how this neuron predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hyjfcthcHupT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=np.array(neuron.forward_pass(X) > 0.5).ravel(), cmap='spring')\n",
    "plt.title('Apples and pears', fontsize=15)\n",
    "plt.xlabel('symmetry', fontsize=14)\n",
    "plt.ylabel('yellowness', fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fA9TCYBXHupW"
   },
   "source": [
    "It should devide more or less not bad. But why we would use ReLU which is used more often and it predicts worse (converges quite a long time), than perceptron with threshold activation which nobody uses? Speaking generally no one knows when and where and which activation function 'will fire'. It depends also on the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDvxxQQDHupZ"
   },
   "source": [
    "<img src=\"https://alumni.lscollege.ac.uk/files/2015/12/Interview-questions-square-image.jpg\" width=400 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSNZuWl9Hupd"
   },
   "source": [
    "But there is actually a trend: threshold and sigmoid (sigmoid in the most cases) are used exactly in **output layers** of neural neural networks in classification tasks -- they are used to predict probabilities of an object to belong to a certain class, while more \"advanced\" activation functions (ReLU and those which are will be down below) are used inside a neural network, i.e. **hidden layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_X3lDHElHupo"
   },
   "source": [
    "However nothing prevents you to use ReLU in output layers and sigmoid inside. Deep Learning -- \"quite experimental\" field: you could make a discovery by your own hands by just changing something negligible, i.e. activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sET710OVHupp"
   },
   "source": [
    "**Advantages of ReLU:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HY3iXGBWHupr"
   },
   "source": [
    "* differentiable (with definition in zero)\n",
    "* no problem of fading out gradient like in sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr_3XwTWHups"
   },
   "source": [
    "**Possible disadvantages of ReLU:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TAwVdRXHupt"
   },
   "source": [
    "* not centered near 0 (can prevent convergement speed)\n",
    "* zeros out all negative inputs, thereby weights of zeroed out neurons may often *not update*, this issue is called *dead neurons*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vj1JGXTPHupu"
   },
   "source": [
    "The last one can be fought:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yn2taDMNHupv"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Neuron with LeakyReLU (Leaky Recitified Linear Unit)</b></h2>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCRBWooSHupx"
   },
   "source": [
    "LeakyReLU little difference with ReLU, but it helps network train faster since there is no problem of \"dead neurons\"\n",
    "\n",
    "\\begin{equation*}\n",
    "LeakyReLU(x) =\n",
    " \\begin{cases}\n",
    "   \\alpha x, &\\text{$x \\le 0$}\\\\\n",
    "   x, &\\text{$x \\gt 0$}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\alpha$ -- small number from 0 to 1.\n",
    "\n",
    "Derivative here is taken in the same way, but in 0 value is $\\alpha$:\n",
    "\n",
    "\\begin{equation*}\n",
    "LeakyReLU'(x) = \n",
    " \\begin{cases}\n",
    "   \\alpha, &\\text{$x \\le 0$}\\\\\n",
    "   1, &\\text{$x \\gt 0$}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Plot of the function:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*UtLlZJ80TMIM7kXk.\" width=400 height=300>\n",
    "\n",
    "Substitude LeakyReLU into Loss:\n",
    "\n",
    "$$\n",
    "Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (LeakyReLU(w \\cdot X_i) - y_i)^2 =\n",
    "\\begin{equation*}\n",
    "\\frac{1}{2n}\\sum_{i=1}^{n} \n",
    " \\begin{cases}\n",
    "   (\\alpha \\cdot w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\le 0}\\\\\n",
    "   (w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\gt 0}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "$$  \n",
    "\n",
    "Formula for updating weights in gradient descend:\n",
    "\n",
    "$$ \\frac{\\partial Loss}{\\partial w} = \\begin{equation*}\n",
    "\\frac{1}{n}\\sum_{i=1}^{n} \n",
    " \\begin{cases}\n",
    "   \\alpha X_i^T (w \\cdot X_i - y), &{w \\cdot X_i \\le 0}\\\\\n",
    "    X_i^T (w \\cdot X_i - y), &{w \\cdot X_i \\gt 0}\n",
    " \\end{cases}\n",
    "\\end{equation*}$$\n",
    "\n",
    "* Implement LeakyReLU and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuBLORHs6reh"
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"LeakyReLU\"\"\"\n",
    "    <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZObir-96rek"
   },
   "outputs": [],
   "source": [
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    \"\"\"Derivative of LeakyReLU\"\"\"\n",
    "    <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKurn-7F9igN"
   },
   "source": [
    "Now you need to code neuron with ReLU. Here it's all similar to Perceptron, but the weights are updated differently and the activation function is different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1hF_ufY6ren"
   },
   "outputs": [],
   "source": [
    "class NeuronReLU:\n",
    "    def __init__(self, w=None, b=0):\n",
    "        \"\"\"\n",
    "        :param: w -- weights vector\n",
    "        :param: b -- bias scalar\n",
    "        \"\"\"\n",
    "        # Let's leave an opportunity for a user to set weights and biases directly\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def activate(self, x):\n",
    "        # You code here\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        This function computes an answer of the perceptron given a set of objects\n",
    "        :param: X -- matrix of objects sized (n, m), every row - separate object\n",
    "        :return: vector sized (n, 1) of zeros and ones containing model answers \n",
    "        \"\"\"\n",
    "        # You code here\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        y_pred = np.zeros((n, 1))  # y_pred == y_predicted - predicted classes\n",
    "        # You code here\n",
    "    \n",
    "    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n",
    "        \"\"\"\n",
    "        Updates weights values given objects\n",
    "        :param: X -- matrix of objects sized (n, m)\n",
    "                y -- right answers vector sized (n, 1)\n",
    "                learning_rate - \"speed of learning\" (symbol alpha in formulas above)\n",
    "        This method doesn't return anything, it only corrects weights using gradient\n",
    "        descend.\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        # You code here\n",
    "    \n",
    "    def fit(self, X, y, num_epochs=300):\n",
    "        \"\"\"\n",
    "        Descend in a minimum\n",
    "        :param: X -- matrix of objects sized (n, m)\n",
    "                y -- right answers vector sized (n, 1)\n",
    "                num_epochs -- number of training steps\n",
    "        :return: Loss_values -- vector of loss values\n",
    "        \"\"\"\n",
    "        #  self.w = np.zeros((X.shape[1], 1))  # column (m, 1)\n",
    "        #  self.b = 0  # bias (number)\n",
    "        Loss_values = []  # loss values on every step of fitting\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            # You code here\n",
    "        \n",
    "        return Loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhbHyAib6req"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Testing neuron with LeakyReLU</b></h3>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chEeb88gHuny"
   },
   "source": [
    "***Note***: please, check `.shape` of matricies and vectors more often: `self.w`, `X` and `y` inside the class. Often mistake is solved with transposition or with method `.reshape()`. Don't forget to check what vector (what size) you want to get as an output -- this quite helps not to get confused.\n",
    "\n",
    "**Everywhere below in testing don't change $\\alpha$=0.01 in `leaky_relu()` and in `leaky_relu_derivative()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsCNxcsDHuqb"
   },
   "source": [
    "\"Apples and pears\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MEXxd-YRHuqg"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/apples_pears.csv\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data['target'], cmap='rainbow')\n",
    "plt.title('Apples and pears', fontsize=15)\n",
    "plt.xlabel('symmetry', fontsize=14)\n",
    "plt.ylabel('yellowness', fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzQyBSMOHuqk"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,:2].values  # matrix objects-features\n",
    "y = data['target'].values.reshape((-1, 1))  # classes (column of zeros and ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-92UxDqHuqo"
   },
   "source": [
    "Let's train the neuron with randomly initialized weights (put 10000 iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGztqTFhHuqo"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "neuron = NeuronLeakyReLU(w=np.random.rand(X.shape[1], 1), b=np.random.rand(1))\n",
    "Loss_values = neuron.fit(X, y, num_epochs=10000)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(Loss_values)\n",
    "plt.title('Loss function', fontsize=15)\n",
    "plt.xlabel('iteration number', fontsize=14)\n",
    "plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3XeufnfHupE"
   },
   "source": [
    "**(for the test) Check loss:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCn5DSP5HupK"
   },
   "source": [
    "Display a summ of the first five and the last five values of loss during training for num_epochs=10000, round to the 4-th decimal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gThPeHNm9Am6"
   },
   "source": [
    "IMPORTANT! If you have run the previous cell with code several times, then to carry the results in the test in Canvas, please, run this cell from \"zero\", restart Runtime, since `random` brings in randomness. There are several possible answers, in case, you have run it several times, but it's better not to carry the result after 10 runs of these and following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QDRx6efHuq1"
   },
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvj3t74RHuq3"
   },
   "source": [
    "Let's see how this neuron predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfFexRvcHuq5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=np.array(neuron.forward_pass(X) > 0.5).ravel(), cmap='spring')\n",
    "plt.title('Apples and pears', fontsize=15)\n",
    "plt.xlabel('symmetry', fontsize=14)\n",
    "plt.ylabel('yellowness', fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgTac_vFHuq8"
   },
   "source": [
    "**Advantages of LeakyReLU:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HY3iXGBWHupr"
   },
   "source": [
    "* differentiable (with definition in zero)\n",
    "* no problem of fading out gradient like in sigmoid\n",
    "* no problem of \"dead neurons\" like in ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr_3XwTWHups"
   },
   "source": [
    "**Possible disadvantages of LeakyReLU:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TAwVdRXHupt"
   },
   "source": [
    "* not centered near 0 (can prevent convergement speed)\n",
    "* little unstable to \"noise\" (see Stanford lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e7k07EGyHurB"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Neuron with ELU (Exponential Linear Unit)</a></b></h2>  \n",
    "<h2 style=\"text-align: center;\"><b>(optional part, will not be checked)</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWH3Zk2zHurB"
   },
   "source": [
    "ELU -- revealed not so long ago (in 2015 year) acrivation function, which, as authors of the paper say, is better than LeakyReLU. Here is a formula for ELU:\n",
    "\n",
    "\\begin{equation*}\n",
    "ELU(\\alpha, x) =\n",
    " \\begin{cases}\n",
    "   \\alpha (e^x - 1), &\\text{$x \\le 0$}\\\\\n",
    "   x, &\\text{$x \\gt 0$}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\alpha$ -- small number from 0 to 1.\n",
    "\n",
    "Derivative here is taken in the same way, but in 0 value is $\\alpha$:\n",
    "\n",
    "\\begin{equation*}\n",
    "ELU'(x) = \n",
    " \\begin{cases}\n",
    "   ELU(\\alpha, x) + \\alpha, &\\text{$x \\le 0$}\\\\\n",
    "   1, &\\text{$x \\gt 0$}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Simple trick is used here in derivative -- added $- \\alpha + \\alpha$, to make computation easier.\n",
    "\n",
    "Plot of this function:\n",
    "\n",
    "<img src=\"http://p0.ifengimg.com/pmop/2017/0907/A004001DD141881BFD8AD62E5D31028C3BE3FAD1_size14_w446_h354.png\" width=500 height=400>\n",
    "\n",
    "Substitute LeakyReLU into Loss:\n",
    "\n",
    "$$Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (ELU(\\alpha, w \\cdot X_i) - y_i)^2 = \\begin{equation*}\n",
    "\\frac{1}{2n}\\sum_{i=1}^{n} \n",
    " \\begin{cases}\n",
    "   (\\alpha (e^{w \\cdot X_i} - 1) - y_i)^2, &{w \\cdot X_i \\le 0}\\\\\n",
    "   (w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\gt 0}\n",
    " \\end{cases}\n",
    "\\end{equation*}$$  \n",
    "\n",
    "Formula for updating weights in gradient descend. Here you need to derive it yourself. And it is a little harded than before. Taking derivative head-on is unconvinient. **Chain rule** is required or **rule of taking derivative of a composition of functions**:\n",
    "\n",
    "$$ \\frac{\\partial Loss}{\\partial w} = \\begin{equation*}\n",
    "\\frac{1}{n}\\sum_{i=1}^{n} \n",
    " \\begin{cases}\n",
    "   , &{w \\cdot X_i \\le 0}\\\\\n",
    "   , &{w \\cdot X_i \\gt 0}\n",
    " \\end{cases}\n",
    "\\end{equation*}$$\n",
    "\n",
    "* Implement ELU and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7rc5PaI_tb4"
   },
   "outputs": [],
   "source": [
    "def elu(x, alpha=0.01):\n",
    "    \"\"\"ELU\"\"\"\n",
    "    <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63ORnClM_0gf"
   },
   "outputs": [],
   "source": [
    "def elu_derivative(x, alpha=0.01):\n",
    "    \"\"\"Derivative of ELU\"\"\"\n",
    "    <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wo5dq-2X_0ze"
   },
   "source": [
    "Now you need to code neuron with ELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9lTuu7gG_06W"
   },
   "outputs": [],
   "source": [
    "class NeuronReLU:\n",
    "    def __init__(self, w=None, b=0):\n",
    "        \"\"\"\n",
    "        :param: w -- weights vector\n",
    "        :param: b -- bias scalar\n",
    "        \"\"\"\n",
    "        # Let's leave an opportunity for a user to set weights and biases directly\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def activate(self, x):\n",
    "        # You code here\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        This function computes an answer of the perceptron given a set of objects\n",
    "        :param: X -- matrix of objects sized (n, m), every row - separate object\n",
    "        :return: vector sized (n, 1) of zeros and ones containing model answers \n",
    "        \"\"\"\n",
    "        # You code here\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        y_pred = np.zeros((n, 1))  # y_pred == y_predicted - predicted classes\n",
    "        # You code here\n",
    "    \n",
    "    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n",
    "        \"\"\"\n",
    "        Updates weights values given objects\n",
    "        :param: X -- matrix of objects sized (n, m)\n",
    "                y -- right answers vector sized (n, 1)\n",
    "                learning_rate - \"speed of learning\" (symbol alpha in formulas above)\n",
    "        This method doesn't return anything, it only corrects weights using gradient\n",
    "        descend.\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        # You code here\n",
    "    \n",
    "    def fit(self, X, y, num_epochs=300):\n",
    "        \"\"\"\n",
    "        Descend in a minimum\n",
    "        :param: X -- matrix of objects sized (n, m)\n",
    "                y -- right answers vector sized (n, 1)\n",
    "                num_epochs -- number of training steps\n",
    "        :return: Loss_values -- vector of loss values\n",
    "        \"\"\"\n",
    "        # self.w = np.zeros((X.shape[1], 1))  # column (m, 1)\n",
    "        # self.b = 0  # bias (number)\n",
    "        Loss_values = []  # loss values on every step of fitting\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            # You code here\n",
    "        \n",
    "        return Loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chEeb88gHuny"
   },
   "source": [
    "***Note***: please, check `.shape` of matricies and vectors more often: `self.w`, `X` and `y` inside the class. Often mistake is solved with transposition or with method `.reshape()`. Don't forget to check what vector (what size) you want to get as an output -- this quite helps not to get confused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pskXX7KVHurf"
   },
   "source": [
    "\"Apples and pears\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSzTCZu_Hurh"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/apples_pears.csv\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data['target'], cmap='rainbow')\n",
    "plt.title('Apples and pears', fontsize=15)\n",
    "plt.xlabel('symmetry', fontsize=14)\n",
    "plt.ylabel('yellowness', fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1upsgBzHurl"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,:2].values  # matrix objects-features\n",
    "y = data['target'].values.reshape((-1, 1))  # classes (column of zeros and ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EsRToko0Hurp"
   },
   "source": [
    "Let's train a neuron but initialize weights at the beginning (do 10000 iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6lOu_FbHurr"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "neuron = NeuronELU(w=np.random.rand(X.shape[1], 1), b=np.random.rand(1))\n",
    "Loss_values = neuron.fit(X, y, num_epochs=10000)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(Loss_values)\n",
    "plt.title('Loss function', fontsize=15)\n",
    "plt.xlabel('iteration number', fontsize=14)\n",
    "plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3XeufnfHupE"
   },
   "source": [
    "**(for the test) Check loss:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCn5DSP5HupK"
   },
   "source": [
    "Display a summ of the first five and the last five values of loss during training for num_epochs=10000, round to the 4-th decimal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yRpg7RMHurv"
   },
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkWMrc9uHupS"
   },
   "source": [
    "Let's see how this neuron predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CW7nCIEyHur2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=np.array(neuron.forward_pass(X) > 0.5).ravel(), cmap='spring')\n",
    "plt.title('Apples and pears', fontsize=15)\n",
    "plt.xlabel('symmetry', fontsize=14)\n",
    "plt.ylabel('yellowness', fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgTac_vFHuq8"
   },
   "source": [
    "**Advantages of ELU:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HY3iXGBWHupr"
   },
   "source": [
    "* differentiable (with definition in zero)\n",
    "* no problem of fading out gradient like in sigmoid\n",
    "* no problem of \"dead neurons\" like in ReLU\n",
    "* more stable to the \"noize\" (see Stanford lectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr_3XwTWHups"
   },
   "source": [
    "**Possible disadvantages of ELU:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TAwVdRXHupt"
   },
   "source": [
    "* not centered near 0 (can prevent convergement speed)\n",
    "* computatioally harder than ReLU and LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_YkH_HDHur7"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQG-VZuUHur8"
   },
   "source": [
    "And finally -- all pokemons (almost):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2JcPTqKHur-"
   },
   "source": [
    "<img src=\"http://cdn-images-1.medium.com/max/1600/1*DRKBmIlr7JowhSbqL6wngg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A82W6KU7HusA"
   },
   "source": [
    "It lacks `SeLU()` and `Swish()`. You can learn more about them: [SeLU](https://arxiv.org/pdf/1706.02515.pdf), [Swish](https://arxiv.org/pdf/1710.05941.pdf).\n",
    "\n",
    "`Tanh()` (hyperbolic tangent) is used rarely, and we decided not to consider `Maxout()` (as, again, we observed that is not usually used, but there are good opinions on it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PfNPtrZ31vq"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EiQGCe9lHusC"
   },
   "source": [
    "Do you think these are all activation functions? No, after all you can use any function you think will help in learning. More activation functions [on Wikipedia](https://en.wikipedia.org/wiki/Activation_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhGNsWPQ9igf"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Useful links</b></h3>\n",
    "\n",
    "0). You must check this artcile by Stanford: http://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "1). Great article on activation functions: https://www.jeremyjordan.me/neural-networks-activation-functions/\n",
    "\n",
    "2). [Video by Siraj Raval](https://www.youtube.com/watch?v=-7scQpJT7uo)\n",
    "\n",
    "3). Modern paper on activation functions. One of the hype functions is $swish(x) = x\\sigma (\\beta x)$: https://arxiv.org/pdf/1710.05941.pdf (by the way, *neural acrhitecture search* was used in search of this function)\n",
    "\n",
    "4). **SeLU** has some interesting properties, proven with probability theory: https://arxiv.org/pdf/1706.02515.pdf (yes, this paper consists of 102 pages)\n",
    "\n",
    "5). [List of activation functions on Wikipedia](https://en.wikipedia.org/wiki/Activation_function)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]activations.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
