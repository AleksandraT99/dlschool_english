{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
    "<h3 style=\"text-align: center;\"><b>PhysTech School of Applied Mathematics and Informatics (FPMI) MIPT </b> </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\"><b>Homework assignment: Dropout on a regression task</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reminder how a fully-connected neural network works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0402.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0402.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is one of the biggest problems of deep networks. It's nature is the following: model overadapts to the training set, becoming extremely good at explaining samples from training dataset, but, meanwhile, it lacks the ability to generalize to the real nature of data, resulting in poor performance on val and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good example of overfitting is on the following image: black dots symbolize training examples, the blue line is a model trained function and the black line is a real function where data comes from (data is just noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/68/Overfitted_Data.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://upload.wikimedia.org/wikipedia/commons/6/68/Overfitted_Data.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned from the lecture, one of the great techniques to fight overfitting is dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training iteration some nodes on a layer with dropout are pretended to be eliminated. Quantity of eliminated nodes on each iteration depend on a dropout parameter -- for example, if you choose it to be 0.2, then 20% of layer nodes will be thrown away each iteration (but each iteration they will be different, chosen randomly). \n",
    "\n",
    "\"Eliminated\" nodes do not take part in computing forwand pass, nor backward pass.\n",
    "\n",
    "On the image below you can see the same network without dropout (left) and with dropout applied (right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://hsto.org/web/dd8/171/16f/dd817116fc2348e78272577153e31d2d.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://hsto.org/web/dd8/171/16f/dd817116fc2348e78272577153e31d2d.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During inference no nodes are thrown away. But the network with dropout is bigger than each network got after dropout, so during inference we need to make calibration of layers outputs. More about that was in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall main features of dropout:\n",
    "\n",
    "<ul> \n",
    "    <li>kind of model ensembling (throwing out some nodes on each iteration results in \"different\" networks trained each iteration)</li>\n",
    "\n",
    "    <li>prevents co-adaptations of the networkâ€™s units so they learn more robust features</li>\n",
    "\n",
    "    <li>can be considered as a kind of regularization for a neural network</li>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. In practice (in PyTorch code) we do not need to worry about \"swithing off\" dropout during inference or making a calibration of layers outputs -- it's all done inside torch. In code we only need to tell torch that we wanna use dropout on a specific layer -- that's all. Well, you'll se it in practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to solve a **regression** task. We'll generate two datasets $test\\_y$, $y$ from one distribution and will train our network on $y$, and use $test\\_y$ as a test set. Of course, we will compare two types of network training: with dropout and without =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "torch.manual_seed(2)   \n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show you how $y$ and $test\\_y$ were generated, but in order to provide correct answers to homework assignments, we've fixed particular arrats of $y$ and $test\\_y$ below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Amount of dots (data samples)\n",
    "N_SAMPLES = 20\n",
    "\n",
    "# Train set\n",
    "x = torch.linspace(-1, 1, N_SAMPLES).view(N_SAMPLES, -1)\n",
    "# y = x + 0.3 * torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "y = torch.tensor([[-1.3122],[-0.6198],[-1.1807],[-1.0171],[-0.5700],[-0.4886],[-0.0489],[ 0.0027],[-0.4012],[ 0.1495]\n",
    "                  ,[-0.2844],[ 0.1303],[ 0.3053],[ 0.7042],[ 0.5683],[ 1.1048],[ 0.4623],[ 0.4167],[ 0.8422],[ 1.2097]])\n",
    "\n",
    "# Test set\n",
    "test_x = torch.linspace(-1, 1, N_SAMPLES).view(N_SAMPLES, -1)\n",
    "#test_y = test_x + 0.3 * torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "test_y = torch.tensor([[-1.3883],[-0.9020],[-0.8601],[-0.8968],[-0.2950],[-1.0038],[-0.5611],[ 0.4919],[-0.4285],[-0.0572],\n",
    "                       [ 0.4683],[ 0.9315],[ 0.1591],[ 0.3946],[ 0.1438],[ 0.7278],[ 1.1072],[ 0.6730],[ 1.0141],[ 0.6687]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter('''your code here''', c='magenta', s=20, alpha=0.6, label='train')\n",
    "plt.scatter('''your code here''', c='cyan', s=20, alpha=0.6, label='test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim((-2.5, 2.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a 3-layer neural network with ReLu as activation function between layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layers size\n",
    "N_HIDDEN = 300\n",
    "net_overfitting = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    <Your code here>\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")\n",
    "print(net_overfitting)  # prints network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the second network let's add Dropout after 1 and 2 layers with keep_rate=0.5 (that means each iteration we'll have 50% nodes eliminated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dropped = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    <Your code here>\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")\n",
    "print(net_dropped) # prints network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers for each of the nets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ofit = torch.optim.Adam(<Your code here>, lr=0.01)\n",
    "optimizer_drop = torch.optim.Adam(<Your code here>, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have mse as a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.<Your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we'll store errors on y and test_y\n",
    "error = []\n",
    "error_test = []\n",
    "for t in range(500):\n",
    "    # forward pass of x\n",
    "    pred_ofit = <Your code here>\n",
    "    pred_drop = <Your code here>\n",
    "    \n",
    "    loss_ofit = loss_func(pred_ofit, y)\n",
    "    loss_drop = loss_func(pred_drop, y)\n",
    "\n",
    "    # gradient descent step\n",
    "    optimizer_ofit.zero_grad()\n",
    "    optimizer_drop.zero_grad()\n",
    "    <Your code here>\n",
    "\n",
    "    # we'll check performance of our nets each 20 steps\n",
    "    if t % 20 == 0:\n",
    "        # we need to switch to eval mode for evaluating\n",
    "        net_overfitting.eval()\n",
    "        net_dropped.eval()\n",
    "\n",
    "        # plotting\n",
    "        clear_output(wait=True)\n",
    "        sleep(0.05)\n",
    "        \n",
    "         # forward pass of x\n",
    "        <Your code here>\n",
    "        \n",
    "        # Plot our data and nets' predictions on test data:\n",
    "        plt.scatter(<Your code here>, c='magenta', s=20, alpha=0.6, label='train')\n",
    "        plt.scatter(<Your code here>, c='cyan', s=20, alpha=0.6, label='test')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_ofit.data.numpy(), 'r-', lw=3, label='overfitting')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_drop.data.numpy(), 'b--', lw=3, label='dropout(50%)')\n",
    "        \n",
    "        # Errors of nets on y and test_y\n",
    "        error.append((loss_func(test_pred_ofit, y).data.numpy(), loss_func(test_pred_drop, y).data.numpy()))\n",
    "        error_test.append(<Your code here>)\n",
    "        \n",
    "        plt.text(-0.5, -1.8, 'overfitting loss=%.4f' % <Your code here>, fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.text(-0.5, -2.3, 'dropout loss=%.4f' % <Your code here>, fontdict={'size': 20, 'color': 'blue'})\n",
    "        plt.legend(loc='upper left'); plt.ylim((-2.5, 2.5));#plt.pause(0.1)\n",
    "        plt.show()\n",
    "        \n",
    "        # switch back to train mode\n",
    "        net_overfitting.train()\n",
    "        net_dropped.train()\n",
    "\n",
    "# Print losses of the networks on y\n",
    "print('overfitting loss train', 'dropout loss train')\n",
    "for i in range(len(error)):\n",
    "    print('%.4f\\t\\t\\t%.4f' % (error[i][0], error[i][1]))\n",
    "\n",
    "print()\n",
    "# Print losses of the networks on test_y\n",
    "print('overfitting loss test', '   dropout loss test')\n",
    "for i in range(len(error)):\n",
    "    print('%.4f\\t\\t\\t%.4f' % '''your code here''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for homework assignments:\n",
    "1. What is an mse error between $y$ Ð¸ $test\\_y$? (using loss_func)\n",
    "<br> Answer: ...\n",
    "2. How many trainable parameters linear hidden layer has? (don't forget about b)?\n",
    "<br> Answer: ...\n",
    "3. What $overfitting\\:loss\\:train$ error did you get after all (round to 2 digits after .)?\n",
    "<br> Answer: ...\n",
    "4. What $dropout\\:loss\\:train$ error did you get after all (round to 2 digits after .)?\n",
    "<br> Answer: ...\n",
    "5. What $overfitting\\:loss\\:test$ error did you get after all (round to 2 digits after .)?\n",
    "<br> Answer: ...\n",
    "6. What $dropout\\:loss\\:test$ error did you get after all (round to 2 digits after .)?\n",
    "<br> Answer : ...\n",
    "\n",
    "<br> Compare 3, 4 and think why you got suchc result\n",
    "<br> Compare 4, 5 and think why you got suchc result\n",
    "<br> Also compare 4, 5 to previous two errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
