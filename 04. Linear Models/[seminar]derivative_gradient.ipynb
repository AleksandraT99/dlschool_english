{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[seminar]derivative_gradient.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"text","id":"RUWCAY5opP87"},"cell_type":"markdown","source":["<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\", width=300, height=300></p>\n","\n","<h3 style=\"text-align: center;\"><b>Phystech School of Applied Mathematics and Informatics (PAMI) MIPT</b></h3>"]},{"metadata":{"id":"xfVT61hO1RDz","colab_type":"text"},"cell_type":"markdown","source":["____________________________________________________________________________________________________"]},{"metadata":{"colab_type":"text","id":"Wj5MrpmRpP89"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b> Elements of optimization theory. Derivatives and partial derivatives.</b></h3>"]},{"metadata":{"colab_type":"text","id":"O1xIGzO5pP8-"},"cell_type":"markdown","source":["<p style=\"text-align: center;\">(Based on: https://github.com/romasoletskyi/Machine-Learning-Course)</p>"]},{"metadata":{"colab_type":"text","id":"ODaZDX75pP8-"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Slope of the linear function</b></h3>"]},{"metadata":{"colab_type":"text","id":"tldAx431pP8_"},"cell_type":"markdown","source":["Let's begin this simple function: $y=kx+b$: <br>  \n","\n","![source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/c/c1/Wiki_slope_in_2d.svg) <br>  \n","\n","We will define **slope** of a funcion in $(x, y)$ as ratio of function change $\\Delta y$ and argument change $\\Delta x$:  \n","\n","$$slope=\\frac{\\Delta y}{\\Delta x}=\\frac{y_2-y_1}{x_2-x_1}=\\frac{kx_2+b-kx_1-b}{x_2-x_1}=k\\frac{x_2-x_1}{x_2-x_1}=k$$  \n","\n","You can see that slope is not a function of $x$ or $\\Delta x$."]},{"metadata":{"colab_type":"text","id":"ObcFoC9JpP9A"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Slope of arbitrary function</b></h3>"]},{"metadata":{"colab_type":"text","id":"dNfWm09WpP9A"},"cell_type":"markdown","source":["What if $f(x)$ is just any function?  \n","In this case it is possible to draw a **tangent line** in any point we are interested in and analyse this line instead of a function. As it is known tangent line is a line and conveniently all math for it is done above.\n","![source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/d/d2/Tangent-calculus.svg)"]},{"metadata":{"colab_type":"text","id":"137KHuBjpP9B"},"cell_type":"markdown","source":["But what if we have an analytic formula for a function and we want to know it's behaviour in any point? Luckily slope evaluation can be formalized. Operation which corresponds to slope evaluation of a function, called derivative:\n","$$f'(x) = \\lim_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x} = \\lim_{\\Delta x \\to 0}\\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$$  \n","\n","This is  limit of a funciton $f$ as $\\Delta x$ approaches 0. This is separate piece of theory which is won't be presented here, but if you want to fully understand derivatives you might want to check it out. "]},{"metadata":{"colab_type":"text","id":"7YksIkmlpP9C"},"cell_type":"markdown","source":["You can use this interactive demo to play with $\\Delta x$ yourself (*It won't work in Google Colab*, so run it locally):"]},{"metadata":{"colab_type":"code","id":"v9rhGojJpP9D","colab":{}},"cell_type":"code","source":["from __future__ import print_function\n","\n","from ipywidgets import interact, interactive, fixed, interact_manual\n","import ipywidgets as widgets\n","\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"R4DbxljwpP9F","colab":{}},"cell_type":"code","source":["#!pip install ipywidgets"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"RJ_xbrHXpP9I","colab":{},"outputId":"b90dc88e-0985-4a85-cd5c-166d1060fda3"},"cell_type":"code","source":["@interact(lg_z=(-0.5,4.0,0.1))\n","def f(lg_z=1.0):\n","    z = 10 ** lg_z\n","    x_min = 1.5 - 6/z\n","    x_max = 1.5 + 6/z\n","    l_min = 1.5 - 4/z\n","    l_max = 1.5 + 4/z\n","    xstep = (x_max - x_min)/100\n","    lstep = (l_max - l_min)/100\n","    \n","    x = np.arange(x_min, x_max, xstep)\n","    \n","    plt.plot(x, np.sin(x), '-b')     \n","    \n","    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n","    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_min)), '-r')\n","    plt.plot((l_max,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n","    \n","    yax = plt.ylim()    \n","    \n","    plt.text(l_max + 0.1/z, (np.sin(l_min) + np.sin(l_max)) / 2, \"$\\Delta y$\")\n","    plt.text((l_min + l_max)/2, np.sin(l_min) - (yax[1]-yax[0]) / 20, \"$\\Delta x$\")\n","    \n","    plt.show()\n","    \n","    print('slope =', (np.sin(l_max) - np.sin(l_min)) / (l_max - l_min))"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02abd069614c45fb828e0c7b44c237db","version_major":2,"version_minor":0},"text/plain":["interactive(children=(FloatSlider(value=1.0, description='lg_z', max=4.0, min=-0.5), Output()), _dom_classes=(…"]},"metadata":{"tags":[]}}]},{"metadata":{"colab_type":"text","id":"v8CYa2CRpP9N"},"cell_type":"markdown","source":["It can be seen that when $\\Delta x$ is getting smaller, slope value changing less and less and in the end converges. Value to which slope converges is called **derivative**. **Derivative** of the fuction $f(x)$ in the point x usually writen down as $f'(x)$ or $\\frac{d}{dx}f(x)$.  "]},{"metadata":{"colab_type":"text","id":"VMwBqnhVpP9N"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Some examples</b></h3>"]},{"metadata":{"colab_type":"text","id":"JwlAAsznpP9P"},"cell_type":"markdown","source":["Let's take some derivatives by definition"]},{"metadata":{"colab_type":"text","id":"6R_rnMsqpP9P"},"cell_type":"markdown","source":["1. $f(x)=x$  \n","\n","$$\\frac{\\Delta y}{\\Delta x}=\\frac{x+\\Delta x-x}{\\Delta x}=1\\Rightarrow \\mathbf{\\frac{d}{dx}(x)=1}$$  \n","\n","2. $f(x)=x^2$  \n","\n","$$\\frac{\\Delta y}{\\Delta x}=\\frac{(x+\\Delta x)^2-x^2}{\\Delta x}=\\frac{x^2+2x\\Delta x+\\Delta x^2-x^2}{\\Delta x}=2x+\\Delta x\\rightarrow 2x (\\Delta x\\rightarrow 0)\\Rightarrow \\mathbf{\\frac{d}{dx}(x^2)=2x}$$  \n","    \n","3. In the general case $f(x)=x^n$ the formula will look like this:  \n","\n","$$\\mathbf{\\frac{d}{dx}(x^n)=nx^{n-1}}$$  "]},{"metadata":{"colab_type":"text","id":"KP4jUOaqpP9P"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Elementary rules of differentiation</b></h3>"]},{"metadata":{"colab_type":"text","id":"8fb0go1lpP9Q"},"cell_type":"markdown","source":["If you will take derivatives by definition you will notice some properties of this operation. This is a list of some useful ones. \n","\n","1). If $f(x) = C$ then it's derivative is zero :  \n","\n","$$(C)' = 0$$\n","\n","2). Derivative is a linear operation:  \n","\n","$$(f(x) + g(x))' = f'(x) + g'(x)$$\n","\n","$$(f(x) - g(x))' = f'(x) - g'(x)$$\n","\n","$$(C*f(x))' = C*g'(x)$$\n","\n","4). Poduct rule:  \n","\n","$$(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$$\n","\n","5). Ratio rule:  \n","\n","$$\\left(\\frac{f(x)}{g(x)}\\right)'=\\frac{f'(x)g(x)-g'(x)f(x)}{g^2(x)}$$\n","\n","6). Chain rule:  \n","\n","$$(f(g(x)))'=f'(g(x))g'(x)$$\n","\n","It alse can be rewritten:  \n","\n","$$\\frac{d}{dx}(f(g(x)))=\\frac{df}{dg}\\frac{dg}{dx}$$"]},{"metadata":{"colab_type":"text","id":"ZFTReW5ppP9R"},"cell_type":"markdown","source":["**Examples**:"]},{"metadata":{"colab_type":"text","id":"grjqr2h4pP9R"},"cell_type":"markdown","source":["* Let's evaluate derivative of  $$f(x) = \\frac{x^2}{cos(x)} + 100$$:  \n","\n","$$f'(x) = \\left(\\frac{x^2}{cos(x)}+100\\right)' = \\left(\\frac{x^2}{cos(x)}\\right)' + (100)' = \\frac{(2x)\\cos(x) - x^2(-\\sin(x))}{cos^2(x)}$$"]},{"metadata":{"colab_type":"text","id":"TSqCaSSYpP9T"},"cell_type":"markdown","source":["* Let's evaluate derivative of $$f(x) = tg(x)$$:  \n","\n","$$f'(x) = \\left(tan(x)\\right)' = \\left(\\frac{\\sin(x)}{\\cos(x)}\\right)' = \\frac{\\cos(x)\\cos(x) - \\sin(x)(-\\sin(x))}{cos^2(x)} = \\frac{1}{cos^2(x)}$$"]},{"metadata":{"colab_type":"text","id":"FXP_YETzpP9T"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Partial derivatives</b></h3>"]},{"metadata":{"colab_type":"text","id":"aJDwZBCZpP9T"},"cell_type":"markdown","source":["If there is function of multiple variables it is harder to image or plot it. If there is more then four axis it is nearly impossible, but lucky for us the rules are still the same. \n","\n","There are some rules though which are new:  \n","$f(\\overline{x}) = f(x_1, x_2, .., x_n)$ - Multivariable function;  \n","Partial derivative of $f$ with respect to $x_i$ is derivative with respect to $x_i$, considering every other variable in the function as constant. \n","\n","More math-like:  \n","\n","Partial derivative of  $f(x_1,x_2,...,x_n)$ with respect to $x_i$ equals to: \n","\n","$$\\frac{\\partial f(x_1,x_2,...,x_n)}{\\partial x_i}=\\frac{df_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)}{dx_i}$$  \n","\n","where $f_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)$ means that  $x_1,...,x_{i-1},x_{i+1},...x_n$ are fixed and you need to treat them as consts."]},{"metadata":{"colab_type":"text","id":"3uPIkZ-wpP9U"},"cell_type":"markdown","source":["**Exapmles**:   "]},{"metadata":{"colab_type":"text","id":"aodpt9VppP9V"},"cell_type":"markdown","source":["* Let's find partial derivates of function $f(x, y) = -x^7 + (y - 2)^2 + 140$ with respect to $x$ and $y$:  \n","\n","$$f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = -7x^6$$  \n","$$f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = 2(y - 2)$$"]},{"metadata":{"colab_type":"text","id":"3pRitR-YpP9W"},"cell_type":"markdown","source":["* Let's find partial derivates of function $f(x, y, z) = \\sin(x)\\cos(y)tg(z)$ with resepct to $x$, $y$ and $z$:  \n","\n","$$f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = \\cos(x)\\cos(y)tg(z)$$  \n","$$f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\sin(x)(-\\sin(y))tg(z)$$\n","$$f_z'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\frac{\\sin(x)\\cos(y)}{\\cos^2{z}}$$"]},{"metadata":{"colab_type":"text","id":"nrmPlYyrpP9X"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Gradient descent</b></h3>"]},{"metadata":{"colab_type":"text","id":"SeD8U4CApP9X"},"cell_type":"markdown","source":["**Gradient** of $f(\\overline{x})$,where $\\overline{x} \\in \\mathbb{R^n}$, i.e $\\overline{x} = (x_1, x_2, .., x_n)$, is a vector of partial derivatives of $f(\\overline{x})$ with respect to corresponding $x_i$ of vector x:  \n","\n","$$grad(f) = \\nabla f(\\overline{x}) = \\left(\\frac{\\partial{f(\\overline{x})}}{\\partial{x_1}}, \\frac{\\partial{f(\\overline{x})}}{\\partial{x_2}}, .., \\frac{\\partial{f(\\overline{x})}}{\\partial{x_n}}\\right)$$"]},{"metadata":{"colab_type":"text","id":"DcjYCHOepP9Y"},"cell_type":"markdown","source":["For example there is $f(x)$ and we want to find the local extremum of this function.\n","\n","Algorithm of the gradient descent:  \n","1. $x^0$ - starting value which is chosen randomly or given by task. (by the way $x^0$ it is not a power it is just an index)\n","2. $x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1})$ where $\\nabla f(x^{i-1})$ - gradient of  $f$ in the point $x^{i-1}$;\n","3. Execute step two until the stop condition is true $||x^{i} - x^{i-1}|| < eps$ where $||x^{i} - x^{i-1}|| = \\sqrt{(x_1^i - x_1^{i-1})^2 + .. + (x_n^i - x_n^{i-1})^2}$.  "]},{"metadata":{"colab_type":"text","id":"zX1miuQ0pP9Z"},"cell_type":"markdown","source":["**Examples:**"]},{"metadata":{"colab_type":"text","id":"t1M6agxdpP9Z"},"cell_type":"markdown","source":["1) Let's evaluate formula for $f(x) = 10x^2$:   "]},{"metadata":{"colab_type":"text","id":"1WMJRqDRpP9a"},"cell_type":"markdown","source":["$x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1}) = x^{i-1} - \\alpha f'(x^{i-1}) = x^{i-1} - \\alpha (20x^{i-1}) = x^{i-1} - 20\\alpha x^{i-1} = -19\\alpha x^{i-1}$"]},{"metadata":{"colab_type":"text","id":"EqjopRZVpP9b"},"cell_type":"markdown","source":["Now we can do something useful i.e implement gradient descent for $f(x) = 10x^2$:"]},{"metadata":{"colab_type":"code","id":"evLahkyIpP9c","colab":{}},"cell_type":"code","source":["import numpy as np\n","from tqdm import tqdm\n","\n","def f(x):\n","    return 10 * x**2\n","\n","def gradient_descent(alpha=0.001, eps=0.01):\n","    x_pred = 100  # init value\n","    x = 50  # init value\n","    for i in range(100000):\n","        if np.sum((x - x_pred)**2) < eps**2:  # stop condition\n","            break\n","        x_pred = x\n","        x = -19 * alpha * x_pred  # implementation of the formula above\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"9k8A7ei8pP9g","colab":{}},"cell_type":"code","source":["x_min = gradient_descent()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"elapsed":636,"status":"ok","timestamp":1538842916473,"user":{"displayName":"Konstantin Baltsat","photoUrl":"","userId":"06917529083593270168"},"user_tz":-180},"id":"uGOVAybRpP9k","outputId":"3a1e65cc-a195-4b67-fa95-17f80f83cd9f","colab":{"base_uri":"https://localhost:8080/","height":156}},"cell_type":"code","source":["x_min"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.51605e-06"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"colab_type":"code","executionInfo":{"elapsed":581,"status":"ok","timestamp":1538842938177,"user":{"displayName":"Konstantin Baltsat","photoUrl":"","userId":"06917529083593270168"},"user_tz":-180},"id":"_EW8AY8VpP9n","outputId":"f7e5c27b-1d62-4c2c-d381-a4dfe394f25c","colab":{"base_uri":"https://localhost:8080/","height":156}},"cell_type":"code","source":["f(x_min)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.24589076025e-10"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"colab_type":"text","id":"tsqfsTezpP9q"},"cell_type":"markdown","source":["2)Let's implement gradient descent for another function $f(x, y) = 10x^2 + y^2$:   "]},{"metadata":{"colab_type":"text","id":"MNWMBcbNpP9r"},"cell_type":"markdown","source":["$$\\left(\\begin{matrix} x^i \\\\ y^i \\end{matrix}\\right) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\nabla f(x^{i-1}, y^{i-1}) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\left(\\begin{matrix} \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{x}} \\\\ \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{y}} \\end{matrix}\\right) = x^{i-1} - \\alpha \\left(\\begin{matrix} 20x^{i-1} \\\\ 2y^{i-1} \\end{matrix}\\right)$$"]},{"metadata":{"colab_type":"code","id":"p_rDsja-pP9s","colab":{}},"cell_type":"code","source":["import numpy as np\n","from tqdm import tqdm\n","\n","def f(x):\n","    return 10 * x[0]**2 + x[1]**2\n","\n","def gradient_descent(alpha=0.01, eps=0.001):\n","    x_prev = np.array([100, 100])  # init value\n","    x = np.array([50, 50])  # init value\n","    for _ in range(100000):\n","        if np.sum((x - x_prev)**2) < eps**2:  # stop condition\n","            break\n","        x_prev = x\n","        x = x_prev - alpha * np.array(20 * x_prev[0], 2 * x_prev[1])  # formula implementation\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"boueQCnXpP9u","colab":{}},"cell_type":"code","source":["x_min = gradient_descent()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"elapsed":869,"status":"ok","timestamp":1538833263482,"user":{"displayName":"Григорий Алексеевич Лелейтнер","photoUrl":"","userId":"07080665896519552124"},"user_tz":-300},"id":"6pyhQsmXpP9x","outputId":"7fbba388-a1b1-4e81-9996-e4e88305bf84","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["x_min"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.00272226, 0.00272226])"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"colab_type":"code","executionInfo":{"elapsed":760,"status":"ok","timestamp":1538833264359,"user":{"displayName":"Григорий Алексеевич Лелейтнер","photoUrl":"","userId":"07080665896519552124"},"user_tz":-300},"id":"ytAfn_X7pP90","outputId":"90d9a8f8-24a7-4741-ba91-af01770726ad","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["f(x_min)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8.151763082307056e-05"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"colab_type":"text","id":"YyKaCWuJpP93"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Homework</b></h3>"]},{"metadata":{"colab_type":"text","id":"jrjiC9mUpP93"},"cell_type":"markdown","source":["1). (for those who found everything above boring due to supreme knowledge) Evaluete derivative of $f(x)=\\frac{1}{x}$ by definition and compare your result with result of using formula for power function;  \n","2). Evaluate derivatives of these functions:  \n","\n","$$f(x)=x^3+3\\sqrt{x}-e^x$$\n","\n","$$f(x)=\\frac{x^2-1}{x^2+1}$$\n","\n","$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n","\n","$$L(y, \\hat{y}) = (y-\\hat{y})^2$$  \n","\n","3). Implement gradient descent algorithm for this function:  \n","$$f(w, x) = \\frac{1}{1 + e^{-wx}}$$  "]},{"metadata":{"colab_type":"text","id":"wxDBOB04pP93"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Usefull links</b></h3>"]},{"metadata":{"colab_type":"text","id":"Nm0VC825pP95"},"cell_type":"markdown","source":["***Derivatives:***\n","\n","For those who wants to learn more about derivatives:  \n","\n","https://www.khanacademy.org/math/differential-calculus/derivative-intro-dc\n","\n","***Optimizations of NN:***\n","\n","Nice animations:\n","www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n","\n","Very nice article about gradient descent algorithms(yes there are more then one):\n","http://ruder.io/optimizing-gradient-descent/"]}]}